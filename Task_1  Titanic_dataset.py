# -*- coding: utf-8 -*-
"""Titanic Dataset .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_pZiDVs018KbC4Nnr-7DLeKBTSSktIY8

# Step 1: Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

"""# Step 2: Load and Explore the Dataset"""

# Load the dataset
df = pd.read_csv('Titanic-Dataset.csv')

"""Display the first few rows of the dataset"""

df.head()

"""Display basic statistics and information"""

print(df.info())
print(df.describe())

"""# Step 3: Data Preprocessing

Fill missing values in 'Age' with the median
"""

imputer = SimpleImputer(strategy='median')
df['Age'] = imputer.fit_transform(df[['Age']])

"""Fill missing values in 'Embarked' with the mode"""

df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

df.head()

"""Drop the 'Cabin' column due to a large number of missing values

"""

df.drop(columns=['Cabin'], inplace=True)

df.head()

"""Drop 'Name', 'Ticket', and 'PassengerId' as they are not useful for the prediction"""

df.drop(columns=['Name', 'Ticket', 'PassengerId'], inplace = True)

"""Convert Categorical variables to numeric using LabelEncoder"""

label_encoder = LabelEncoder()
df['Sex'] = label_encoder.fit_transform(df['Sex'])
df['Embarked'] = label_encoder.fit_transform(df['Embarked'])

df.head()

"""Feature engineering: Create a new feature 'IsAlone'"""

df['IsAlone'] = (df['SibSp'] + df['Parch'] == 0).astype(int)

df.drop(columns = ['SibSp', 'Parch'], inplace=True)

df.head()

"""Scale the features"""

scaler = StandardScaler()
df[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])

# Display the processed dataset
print(df.head())

"""# Step 4: Split the Data

Define features and target variable
"""

X = df.drop('Survived', axis=1)
y = df['Survived']

"""Split the dataset into training and testing sets

"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Step 5: Train the Random Forest Model

Initialize the RandomForestClassifier
"""

model = RandomForestClassifier(random_state = 42)

"""Hyperparameter tuning using GridSearchCV"""

param_grid = {
    'n_estimators': [50, 100, 200, 300, 400, 500],
    'max_depth': [None, 10, 20, 30, 40, 50, 60],
    'min_samples_split': [2, 5, 10, 20, 30, 50]
}

grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

"""Best model

"""

best_model = grid_search.best_estimator_

"""Train the best model"""

best_model.fit(X_train, y_train)

"""Make predictions"""

y_pred = best_model.predict(X_test)

"""Accuracy"""

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

"""
Classification report"""

print(classification_report(y_test, y_pred))

"""Confusion matrix"""

conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""# Step 7: Feature Importance

Feature importance
"""

importances = best_model.feature_importances_
feature_names = X.columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importance")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), feature_names[indices], rotation=90)
plt.tight_layout()
plt.show()